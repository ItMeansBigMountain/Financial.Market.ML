{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: LONG - SHORT TERM MEMORY\n",
    "- run the HMM notebook to save data for this code's reference\n",
    "   -  `Financial.Market.ML\\1_Unsupervised\\2-Hidden-Markov-Models-In-Practice.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>12_MA</th>\n",
       "      <th>21_MA</th>\n",
       "      <th>HMM</th>\n",
       "      <th>lrets_bench</th>\n",
       "      <th>lrets_strat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>467.890015</td>\n",
       "      <td>467.940002</td>\n",
       "      <td>472.971667</td>\n",
       "      <td>469.719048</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000940</td>\n",
       "      <td>0.000128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>467.950012</td>\n",
       "      <td>466.089996</td>\n",
       "      <td>473.224167</td>\n",
       "      <td>469.555715</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003961</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open       Close       12_MA       21_MA  HMM  lrets_bench  \\\n",
       "0  467.890015  467.940002  472.971667  469.719048    1    -0.000940   \n",
       "1  467.950012  466.089996  473.224167  469.555715    0    -0.003961   \n",
       "\n",
       "   lrets_strat  \n",
       "0     0.000128  \n",
       "1    -0.000000  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data-storage\\HMM-SPY.csv\")\n",
    "useful_features = [\"Open\", \"Close\", \"12_MA\" , \"21_MA\" ,  \"HMM\" , \"lrets_bench\" , \"lrets_strat\"  ]\n",
    "df = df[useful_features]\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>12_MA</th>\n",
       "      <th>21_MA</th>\n",
       "      <th>HMM</th>\n",
       "      <th>lrets_bench</th>\n",
       "      <th>lrets_strat</th>\n",
       "      <th>lrets_bench_rolling</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>453.750000</td>\n",
       "      <td>446.750000</td>\n",
       "      <td>464.206665</td>\n",
       "      <td>468.203333</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.011130</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.004728</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>445.559998</td>\n",
       "      <td>437.980011</td>\n",
       "      <td>460.909167</td>\n",
       "      <td>467.009048</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.019826</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.006617</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>432.029999</td>\n",
       "      <td>439.839996</td>\n",
       "      <td>458.530833</td>\n",
       "      <td>465.682857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005797</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>433.059998</td>\n",
       "      <td>434.470001</td>\n",
       "      <td>455.741666</td>\n",
       "      <td>463.962381</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.012284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006901</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>440.720001</td>\n",
       "      <td>433.380005</td>\n",
       "      <td>453.015834</td>\n",
       "      <td>461.872857</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.002512</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open       Close       12_MA       21_MA  HMM  lrets_bench  \\\n",
       "9   453.750000  446.750000  464.206665  468.203333    0    -0.011130   \n",
       "10  445.559998  437.980011  460.909167  467.009048    0    -0.019826   \n",
       "11  432.029999  439.839996  458.530833  465.682857    1     0.004238   \n",
       "12  433.059998  434.470001  455.741666  463.962381    1    -0.012284   \n",
       "13  440.720001  433.380005  453.015834  461.872857    3    -0.002512   \n",
       "\n",
       "    lrets_strat  lrets_bench_rolling  TARGET  \n",
       "9          -0.0            -0.004728    -0.0  \n",
       "10         -0.0            -0.006617    -0.0  \n",
       "11          0.0            -0.005797    -0.0  \n",
       "12          0.0            -0.006901     0.0  \n",
       "13         -0.0            -0.008059     0.0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"lrets_bench_rolling\"] = df[\"lrets_bench\"].rolling(window=10).mean()\n",
    "\n",
    "\n",
    "# TARGET: PROJECT IF WE CAN PREDICT TOMORROWS LOGARITHMIC RETURNS\n",
    "df[\"TARGET\"] = df[\"lrets_strat\"].shift(1)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Stationaries found:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Open', 'Close', '12_MA', '21_MA']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify non stationary columns \n",
    "non_stationaries = []\n",
    "columns_with_constants = []\n",
    "\n",
    "for col in df.columns:\n",
    "    # Find variance in column numbers\n",
    "    dftest = adfuller(df[col].values)\n",
    "    \n",
    "    # Get p-value\n",
    "    p_value = dftest[1]\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_test = dftest[0] < dftest[4][\"1%\"]\n",
    "    \n",
    "    # Check if non stationary\n",
    "    if p_value > 0.05  or not t_test:\n",
    "        non_stationaries.append(col)\n",
    "\n",
    "print(f\"Non-Stationaries found:  {len(non_stationaries)}\")\n",
    "non_stationaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Non-Stationary items to logorithm\n",
    "df[non_stationaries] =  np.log(df[non_stationaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>12_MA</th>\n",
       "      <th>21_MA</th>\n",
       "      <th>HMM</th>\n",
       "      <th>lrets_bench</th>\n",
       "      <th>lrets_strat</th>\n",
       "      <th>lrets_bench_rolling</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.947813</td>\n",
       "      <td>0.873720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340227</td>\n",
       "      <td>0.608393</td>\n",
       "      <td>0.376253</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.881888</td>\n",
       "      <td>0.796902</td>\n",
       "      <td>0.970475</td>\n",
       "      <td>0.989619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251449</td>\n",
       "      <td>0.608393</td>\n",
       "      <td>0.289945</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.770278</td>\n",
       "      <td>0.813322</td>\n",
       "      <td>0.949048</td>\n",
       "      <td>0.978061</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.497113</td>\n",
       "      <td>0.608393</td>\n",
       "      <td>0.327414</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.778897</td>\n",
       "      <td>0.765725</td>\n",
       "      <td>0.923778</td>\n",
       "      <td>0.963017</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.328442</td>\n",
       "      <td>0.608393</td>\n",
       "      <td>0.276967</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.842357</td>\n",
       "      <td>0.755992</td>\n",
       "      <td>0.898932</td>\n",
       "      <td>0.944670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428206</td>\n",
       "      <td>0.608393</td>\n",
       "      <td>0.224052</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open     Close     12_MA     21_MA       HMM  lrets_bench  \\\n",
       "9   0.947813  0.873720  1.000000  1.000000  0.000000     0.340227   \n",
       "10  0.881888  0.796902  0.970475  0.989619  0.000000     0.251449   \n",
       "11  0.770278  0.813322  0.949048  0.978061  0.333333     0.497113   \n",
       "12  0.778897  0.765725  0.923778  0.963017  0.333333     0.328442   \n",
       "13  0.842357  0.755992  0.898932  0.944670  1.000000     0.428206   \n",
       "\n",
       "    lrets_strat  lrets_bench_rolling  TARGET  \n",
       "9      0.608393             0.376253    -0.0  \n",
       "10     0.608393             0.289945    -0.0  \n",
       "11     0.608393             0.327414    -0.0  \n",
       "12     0.608393             0.276967     0.0  \n",
       "13     0.608393             0.224052     0.0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df.iloc[: , :-1] = scaler.fit_transform(df.iloc[:,:-1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split x & y axis data\n",
    "x_data = df.iloc[: , : -1].values\n",
    "y_data = df.iloc[: , -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SEQUENCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(input_data , n_steps):\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(0 , len(input_data) , 1):\n",
    "        \n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(input_data) -1 :\n",
    "            break\n",
    "\n",
    "        # Gather input and output parts of the pattern\n",
    "        seq_x = input_data[i:end_ix, :-1]\n",
    "\n",
    "        # gather input a\n",
    "        X.append(seq_x)\n",
    "\n",
    "    return np.array(X)\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCTURING SEQUENCE ROLLING PREDICTION \n",
    "- feed the last 8 days and project what the 9th day `Y VALUE` will be\n",
    "- then feed shift down one day and repeat \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of time steps in each sample window\n",
    "n_timesteps = 8\n",
    "\n",
    "# split X into sequence samples \n",
    "X = split_sequence(x_data , n_timesteps)\n",
    "\n",
    "# adjust y to start at the right point\n",
    "y = y_data[n_timesteps: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch DATASET structure class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.float)\n",
    "        self.length = self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "\n",
    "# INIT DATA STRUCTURE WITH ROLLING PREDICTION MODEL\n",
    "trainset = dataset(X , y)\n",
    "trainloader = DataLoader(trainset , batch_size=64  , shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTRUCT NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self , input_dim, hidden_dim, layer_dim , output_dim):\n",
    "        super(LSTM_Net , self).__init__()\n",
    "\n",
    "        # Hidden Dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Hidden Layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim , hidden_dim , layer_dim , batch_first=True)\n",
    "\n",
    "        # Fully Connected Layer 1\n",
    "        self.fc1 = nn.Linear(hidden_dim, 5)\n",
    "\n",
    "        # Fully Connected Layer 2\n",
    "        self.fc2 = nn.Linear(5 , output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # INIT HIDDEN STATE WITH ZEROS (layer_dim = 1 , x.size = 64)\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0) , self.hidden_dim))\n",
    "\n",
    "        # INIT CELL STATE WITH ZEROS\n",
    "        c0 = Variable(torch.zeros(self.layer_dim, x.size(0) , self.hidden_dim))\n",
    "\n",
    "\n",
    "        # CLASSIFICATION MODEL\n",
    "        # out, (hn,cn) = self.lstm(x, (h0,c0))\n",
    "        # out = self.fc1(out[: , -1, : ])\n",
    "        # out = torch.sigmoid(self.fc2(out))\n",
    "\n",
    "        # REGRESSION MODEL - Propagate input through LSTM\n",
    "        ula , (h_out, _) = self.lstm(x, (h0,c0))\n",
    "        h_out = h_out.view(-1,self.hidden_dim)\n",
    "        out = self.fc1(h_out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT NEURAL NNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT MODEL\n",
    "input_dim = X.shape[2]\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 1\n",
    "model = LSTM_Net( input_dim, hidden_dim, layer_dim , output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 7])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([5, 100])\n",
      "torch.Size([5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1])\n",
      "Amount of model params:  8\n"
     ]
    }
   ],
   "source": [
    "# review model structure\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "\n",
    "print(\"Amount of model params: \", len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0216, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0221, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2452e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.9430e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.9479e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.7478e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.4351e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8481e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.8915e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3119e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8160e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0896e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.0706e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9984e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6194e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9878e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1731e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0948e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0194e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1818e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0426e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0787e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0296e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0202e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4035e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0198e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1218e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0254e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9663e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9947e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9397e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9836e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5691e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.0932e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1299e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3574e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3116e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0073e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0326e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1979e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9120e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8046e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7529e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6250e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4859e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9136e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2614e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.3774e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9735e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6094e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8961e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9620e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1849e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3338e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0063e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0727e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1044e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0258e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0277e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0721e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0484e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9295e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8497e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8138e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4617e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9431e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8536e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8623e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2898e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8092e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8187e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7948e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4865e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8150e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.5127e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7696e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.8866e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0374e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2837e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6435e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1333e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9704e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4729e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3860e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1342e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8226e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9117e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5135e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.9379e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1465e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8084e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4490e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4242e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3781e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0848e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1385e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.5959e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4582e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8855e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3622e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1043e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2893e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1480e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6450e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2772e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.4636e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1493e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.7451e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7401e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.6942e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7886e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1317e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5920e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2621e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7327e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3735e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.9854e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7560e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2687e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5959e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1173e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2291e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.0555e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7760e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.3679e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.5707e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.0005e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8676e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.8943e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1714e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2693e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3012e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9978e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.1095e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3131e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1693e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4333e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8134e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0363e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1614e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7491e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2386e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.8526e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4735e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7033e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2847e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2534e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4357e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.5161e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0104e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1453e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.1996e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2021e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5679e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0109e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8034e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2878e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.7884e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2959e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9174e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0986e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2400e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4202e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.2371e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.4166e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8033e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4559e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.9863e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.9512e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2058e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.4046e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.0312e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8840e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2210e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.5335e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.2171e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.8268e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4619e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.7192e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8648e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9839e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.9711e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.7576e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8603e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.7409e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.7031e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0798e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7598e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7563e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4143e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.6298e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.6708e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4508e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2292e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7119e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3625e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.8945e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.0992e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.5801e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1106e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1357e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.3995e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4121e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3188e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2033e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.7750e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4327e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4962e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.3357e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6327e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9450e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.1633e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.4017e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.0319e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.8447e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8260e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7969e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2554e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6297e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8290e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4640e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1149e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8584e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7302e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9654e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5042e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3134e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.7643e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3304e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0703e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7177e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5527e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6322e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0956e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.8198e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3276e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4311e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.9860e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.9179e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1303e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8939e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8698e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6171e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1408e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3989e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.9737e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8171e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6471e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1617e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.4333e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.9544e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0107e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0453e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.3021e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3103e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.9481e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7351e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2193e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.2094e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2219e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3828e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3365e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7872e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0781e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0767e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.4957e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1342e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2029e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1196e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.4833e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.6844e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.0029e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7626e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.7601e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0432e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1509e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3701e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.0377e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.4193e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8118e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0810e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.3523e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1720e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3656e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4509e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0242e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5015e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.4359e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1701e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.4238e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8527e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5898e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9687e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6210e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2112e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.5975e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8253e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0897e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2367e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.6122e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.6747e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5622e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7680e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9246e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2818e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5400e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3028e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6247e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2703e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.7443e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6354e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9982e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5339e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5779e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.7549e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.6333e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.1752e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.4750e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.2321e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1698e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5205e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3382e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5118e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7487e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.4941e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1588e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1214e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8320e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2559e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1078e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2277e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6761e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.4307e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2908e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1624e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1722e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4439e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6258e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5469e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2363e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3668e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9046e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.9988e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.0535e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.8389e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.9920e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.0033e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.1493e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.3390e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.4605e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.8713e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.3859e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.2142e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.7400e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1349e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4456e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3907e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5535e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.7639e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.1376e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.5924e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.6621e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.0855e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.7949e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9243e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9050e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3595e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.0538e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.5404e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3145e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.9012e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2698e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3576e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5794e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.6606e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1349e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2069e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2314e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3249e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1087e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0774e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1370e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.5833e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0541e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0248e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2930e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.9001e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.0789e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9824e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.1624e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1646e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0986e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3411e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8020e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.3389e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.5609e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.4537e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.5425e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2996e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9454e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.8049e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5378e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9194e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.1676e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.6598e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5581e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2734e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3556e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.4232e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.6450, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5116, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1448, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9171, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.5259e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.6124e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2049e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5822e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3337e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2055e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3504e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3995e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3351e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2781e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2605e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2689e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2865e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2995e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3037e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3034e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3038e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3070e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3116e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3162e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3200e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3232e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3264e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3299e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3336e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3371e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3407e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3442e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3478e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3513e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3548e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3584e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3620e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3656e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3692e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3729e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3765e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3801e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3838e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3875e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3913e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3950e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3988e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4024e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4063e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4102e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4139e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4177e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4215e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4253e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4292e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4330e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4367e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4403e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4441e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4478e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4513e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4548e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4585e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4619e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4650e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4683e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4715e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4744e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4772e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4800e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4826e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4849e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4872e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4893e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4912e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4930e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4944e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4958e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4967e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4975e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4980e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4982e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4984e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4981e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4973e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4964e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4953e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4937e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4918e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4894e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4867e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4834e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4798e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4754e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4697e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4607e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4375e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4156e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4241e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4316e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4085e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4004e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4109e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4019e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3857e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3856e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3849e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3709e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3609e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3575e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3485e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3354e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3258e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3177e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3088e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3021e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3012e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3073e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3207e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3409e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3709e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4396e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7272e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6117e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2779e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7458e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3653e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1424e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.0571e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4016e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7022e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8278e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2735e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2496e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3457e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3984e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2152e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1700e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2598e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4478e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5052e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6748e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.0565e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2977e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.2780e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4330e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2325e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.6317e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2073e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2092e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7368e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5815e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4808e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6837e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6288e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4021e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4388e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7371e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6428e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2657e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1829e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1072e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7875e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4015e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1640e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1186e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1802e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3105e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1650e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9605e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1102e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7785e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1539e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.5798e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5780e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7846e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2411e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2398e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3231e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5417e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3829e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1718e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2259e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9367e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5517e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.5719e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2576e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3792e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2222e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5487e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1702e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1592e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6157e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5571e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2041e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1313e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1859e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1710e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8050e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1979e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.7757e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1823e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8007e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.6577e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6147e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.3510e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4763e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5172e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4071e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9144e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3414e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2501e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4330e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4406e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3885e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3820e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3684e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3624e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3702e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3647e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3536e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3368e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3199e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4456e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4675e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1841e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0720e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2486e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6141e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4238e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2554e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9282e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.8661e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8878e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.9630e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1981e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5947e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1933e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9541e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4609e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2056e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3098e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3003e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2891e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3309e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2973e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2457e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2426e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2639e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2593e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2166e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1818e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1852e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2178e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2891e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6861e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2426e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1368e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.7873e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3835e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.2108e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9690e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1872e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6285e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1198e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4260e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6132e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2109e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2593e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2374e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1783e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2060e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2160e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1856e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1635e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1465e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1243e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0980e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0699e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0453e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0559e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2648e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9455e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8774e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.0152e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3054e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.0183e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0367e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.8183e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1384e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3200e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3800e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1124e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1052e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4450e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2095e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1320e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1611e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1150e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0745e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0627e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0512e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0458e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1057e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2625e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4391e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7442e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3343e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3843e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.6514e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3582e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.4997e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0513e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1235e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5867e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0597e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1668e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1772e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0854e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1411e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1229e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0849e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0856e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3850e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9219e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9117e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9604e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7100e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0000e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6308e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5333e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9794e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2641e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.0414e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.7673e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8809e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6999e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6903e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0893e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3091e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4533e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1842e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.3296e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6841e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8218e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7045e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1448e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.5658e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.0153e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8071e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6804e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3224e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5046e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0944e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8062e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3819e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9155e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6786e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.3515e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9822e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1628e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4726e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9668e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2170e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0296e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9795e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0026e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8757e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7133e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5604e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4222e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.9443e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.3847e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.2107e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8845e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.2663e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1032e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2694e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5467e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1271e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4484e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6042e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.7258e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1710e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6625e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4445e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.1768e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.4548e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1892e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3449e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7574e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2789e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.9221e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4237e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3014e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2110e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0268e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1164e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.1103e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.6920e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1863e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.9048e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.8102e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5138e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5854e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3435e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5203e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.7544e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.5929e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.0626e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.1508e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3862e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4622e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1254e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1171e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8813e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.0531e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5646e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.5192e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.3604e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3343e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.5989e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.7816e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1344e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2151e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.7190e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2558e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4848e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2966e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8313e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5932e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1401e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.2953e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.6847e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3005e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9018e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4668e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.9698e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.1690e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.3874e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.0740e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1252e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3368e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6642e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.2020e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.9581e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4652e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.1277e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.9491e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2414e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.8260e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7750e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6781e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.7202e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.8341e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9884e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4355e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6551e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3582e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0534e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.7972e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5413e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4365e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9682e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.6109e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2038e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7911e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4502e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5113e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9325e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5099e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2491e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5419e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0509e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8223e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0632e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1004e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2809e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0287e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1495e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2648e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8361e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.5530e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1826e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5530e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.1102e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4000e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.4499e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8840e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7236e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8870e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8746e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.0027e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8044e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1722e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.6719e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1484e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2044e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6058e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4631e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(9.2641e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1245e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.7727e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1578e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.8670e-06, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "# # CLASSIFICATION (binary cross entropy loss)\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=le-1)\n",
    "\n",
    "\n",
    "# REGRESSION (mean squared error)\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters() , lr = 1e-1) \n",
    "\n",
    "\n",
    "iter = 0\n",
    "losses = []\n",
    "for epoch in range(0 , epochs , 1):\n",
    "    for i , (X_train, y_train) in enumerate(trainloader):\n",
    "\n",
    "        # Clear Gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(X_train)\n",
    "\n",
    "        # Calculate Loss \n",
    "        loss = criterion(outputs, y_train.unsqueeze(dim=1))\n",
    "\n",
    "        # Backwards Propegation\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating Parameters \n",
    "        optimizer.step()\n",
    "\n",
    "    # TRACK PROGRESS\n",
    "    if iter % 100 == 0:\n",
    "        print(\"Loss\" , loss)\n",
    "        losses.append(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE LOSSES \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
