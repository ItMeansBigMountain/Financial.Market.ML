{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selecection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Remove Future Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data Management\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "\n",
    "# BINARY CLASSIFICATION METRICS\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Reporting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from xgboost import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH DATA\n",
    "df = pd.read_csv(\"data/BTC-USD.csv\")\n",
    "df.set_index(\"Date\" , inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Prediction Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Target     (if yesterdays range was greater than the average range)\n",
    "df.loc[df[\"Range\"].shift(-1) > df[\"AVG_Range\"] , \"TARGET\"] = 1\n",
    "df.loc[df[\"Range\"].shift(-1) <= df[\"AVG_Range\"] , \"TARGET\"] = 0\n",
    "\n",
    "# Check for NAN\n",
    "nan_location = np.where(np.isnan(df))\n",
    "\n",
    "# Fill NaNs\n",
    "df[\"TARGET\"].fillna(0,inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH RECOMMENDED FEATURES FROM FEATURE SELECTION\n",
    "with open('./data/feature_selection_output.json', 'r') as f:\n",
    "    recommended_labels = json.load(f)\n",
    "recommended_labels.append(\"TARGET\")\n",
    "recommended_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE UNWANTED COLUMNS (dont put information about the future in your data other than the target)\n",
    "df_tts = df.copy()\n",
    "df_tts = df_tts[recommended_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVERYTHING EXCEPT TARGET (x)\n",
    "X = df_tts.iloc[: , : -1]\n",
    "\n",
    "# TARGET (y)\n",
    "y = df_tts.iloc[: , -1]\n",
    "df_tts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TEST SPLIT (time series)\n",
    "\n",
    "\n",
    "# 70% OF THE HISTORICAL DATA GOES TO TRAINING THE MODEL\n",
    "train_amount_percent = 0.7\n",
    "train_size = int(len(X) * train_amount_percent)\n",
    "X_train = X.head(train_size)\n",
    "y_train = y.head(train_size)\n",
    "\n",
    "\n",
    "# THE REST GOES TO TESTING THE MODEL FOR ACCURACY\n",
    "test_size = len(X) - train_size\n",
    "X_test = X.tail(test_size)\n",
    "y_test = y.tail(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_check = len(y_test) + len(y_train) == len(X)\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print()\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "print()\n",
    "print(f\"Testing data size check: {size_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select type of model to optimize for\n",
    "is_binary = True\n",
    "is_optimize_for_precision = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Objective and Eval Metrics\n",
    "if is_binary:\n",
    "    objective = \"binary:logistic\"\n",
    "    eval_metric = \"logloss\"\n",
    "    eval_metric_list = [\"error\",\"logloss\",eval_metric]\n",
    "else:\n",
    "    objective = \"multi:softmax\"\n",
    "    eval_metric = \"mlogloss\"\n",
    "    eval_metric_list = [\"merror\",\"mlogloss\",eval_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine Eval Metric\n",
    "if is_binary and is_optimize_for_precision:\n",
    "    eval_metric = \"aucpr\"\n",
    "    scoring = \"precision\"\n",
    "elif is_binary and not is_optimize_for_precision:\n",
    "    eval_metric = \"auc\"\n",
    "    scoring = \"f1\"\n",
    "else:\n",
    "    scoring = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters generated in feature selection\n",
    "\n",
    "\n",
    "with open('./data/hyperparametertuning.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    ne = data['ne']\n",
    "    lr = data['lr']\n",
    "    md = data['md']\n",
    "    gm = data['gm']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build First Classifier Model\n",
    "classifier = XGBClassifier(\n",
    "    objective=objective,\n",
    "    booster=\"gbtree\",\n",
    "    # eval_metric=eval_metric,\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree=1,\n",
    "    random_state=1,\n",
    "    use_label_encoder=False,\n",
    "\n",
    "    n_estimators=ne,\n",
    "    learning_rate=lr,\n",
    "    max_depth=md,\n",
    "    gamma=gm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "eval_set = [(X_train,y_train) , (X_test,y_test)]\n",
    "\n",
    "classifier.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_metric=eval_metric_list,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE PREDICTIONS\n",
    "- binary if the next day will be up (1) or down (0) the next day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "next_day_target_predictions = classifier.predict(X_train)\n",
    "train_yhat_probability = classifier.predict_proba(X_train)\n",
    "\n",
    "\n",
    "# OUTPUT\n",
    "prediction_probabilities = zip(next_day_target_predictions , train_yhat_probability)\n",
    "print(\"Predcition \\t|\\t Binary Confidence (zero or one)\")\n",
    "print(\"___________________________________________________\\n\")\n",
    "for prediction , probability_confidence in list(prediction_probabilities)[:10]:\n",
    "    print(prediction, \"\\t\\t|\\t\",  probability_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Y HAT\n",
    "test_next_day_target_predictions = classifier.predict(X_test)\n",
    "test_yhat_probability = classifier.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# OUTPUT\n",
    "prediction_probabilities = zip(test_next_day_target_predictions , test_yhat_probability)\n",
    "print(\"Predcition \\t|\\t Binary Confidence (zero or one)\")\n",
    "print(\"___________________________________________________\\n\")\n",
    "for prediction , probability_confidence in list(prediction_probabilities)[:10]:\n",
    "    print(prediction, \"\\t\\t|\\t\",  probability_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K FOLD CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD CROSS VALIDATION\n",
    "cv = RepeatedStratifiedKFold(n_splits=5 , n_repeats=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training results \n",
    "train_results = cross_val_score(classifier, X_train, y_train , cv=cv , n_jobs=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUTION METRICS - Loss & Overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
